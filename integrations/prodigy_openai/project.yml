title: "Using Prodigy's OpenAI recipes for a bio NER task"
description: |
  This project showcases Prodigy's OpenAI recipe for named-entity recognition
  using the [Anatomical Entity Mention (AnEM)
  dataset](https://aclanthology.org/W12-4304/).  The dataset contains 11
  anatomical entities (e.g., *organ*, *tissue*, *cellular component*, etc.)
  based from the Common Anatomy Reference Ontology. The dataset statistics (and
  some examples) are shown below:

  <!-- TODO: insert dataset statistics -->

  In this project, we trained a transformer-based NER model and compared it with the zero-shot
  predictions of GPT-3. We wanted to test how large language models fare in a specific domain and
  suggest ways on how we can leverage them to improve our annotations. 

  <!-- TODO: insert zero-shot and supervised learning diagrams -->
  <!-- TODO: insert results -->

  The transformer and zero-shot pipelines are defined by the `ner` and `gpt` workflows respectively.
  In order to run the `gpt` workflow, make sure to [install Prodigy](https://prodi.gy/docs/install) as well
  as a few additional Python dependencies:

  ```
  python -m pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy
  python -m pip install -r requirements.txt
  ```

  With `XXXX-XXXX-XXXX-XXXX` being your personal Prodigy license key.

  Then, [create a new API key from
  openai.com](https://platform.openai.com/account/api-keys) or fetch an existing
  one. Record the secret key as well as the organization key and make sure these
  are available as environmental variables. For instance, set them in a `.env`
  file in the root directory:

  ```
  PRODIGY_OPENAI_ORG = "org-..."
  PRODIGY_OPENAI_KEY = "sk-..."
  ```

directories:
  - "assets"
  - "configs"
  - "corpus"
  - "metrics"
  - "openai"
  - "scripts"
  - "training"

vars:
  gpu_id: 0
  prodigy_dataset: "prodigy_openai"
  config: "ner.cfg"
  prompt_template: "ner_prompt.jinja2"
  seed: 42

assets:
  - dest: "assets/span-labeling-datasets"
    description: "The span-labeling-datasets repository that contains loaders for AnEM"
    git:
      repo: "https://github.com/explosion/span-labeling-datasets/"
      branch: "master"
      path: ""

workflows:
  ner:
    - get-dataset
    - train
    - evaluate
  gpt:
    - openai-preprocess
    - openai-predict
    - openai-evaluate

commands:
  - name: "get-dataset"
    help: "Preprocess the AnEM dataset"
    script:
      - sh -c '(cd assets/span-labeling-datasets && spacy project assets)'
      - sh -c '(cd assets/span-labeling-datasets && spacy project run anem)'
      - cp -a ./assets/span-labeling-datasets/corpus/ner/. ./corpus/.
    deps:
      - assets/span-labeling-datasets
    outputs:
      - corpus/anem-train.spacy
      - corpus/anem-dev.spacy
      - corpus/anem-test.spacy

  - name: "train"
    help: "Train a NER model from the AnEM corpus"
    script:
      - >-
        python -m spacy train
        configs/${vars.config}
        --output training/
        --paths.train corpus/anem-train.spacy
        --paths.dev corpus/anem-dev.spacy
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/anem-train.spacy
      - corpus/anem-dev.spacy
    outputs:
      - training/model-best

  - name: "evaluate"
    help: "Evaluate results for the NER model"
    script:
      - >-
        python -m spacy evaluate
        training/model-best corpus/anem-test.spacy
        --output metrics/scores.json
        --gpu-id ${vars.gpu_id}
    deps:
      - training/model-best/
      - corpus/anem-test.spacy
    outputs:
      - metrics/scores.json

  - name: "openai-preprocess"
    help: "Convert from spaCy format into JSONL."
    script:
      - python -m scripts.convert_to_jsonl corpus/anem-test.spacy corpus/anem-test_texts.jsonl
    deps:
      - corpus/anem-test.spacy
    outputs:
      - corpus/anem-test_texts.jsonl

  - name: "openai-predict"
    help: "Fetch zero-shot NER results using Prodigy's GPT-3 integration"
    script:
      - >-
        python -m prodigy ner.openai.fetch corpus/anem-test_texts.jsonl openai/zeroshot_preds.jsonl
        --prompt-path openai/templates/${vars.prompt_template}
        --labels Cell,Organism_substance,Pathological_formation,Multitissue_structure,Organism_subdivision,Organ,Cellular_component,Anatomical_system,Tissue,Developing_anatomical_structure,Immaterial_anatomical_entity 
        -F ./openai/recipes/ner.py
    deps:
      - corpus/anem-test_texts.jsonl
    outputs:
      - openai/zeroshot_preds.jsonl

  - name: "openai-correct"
    help: "Correct zero-shot NER results using Prodigy's GPT-3 integration"
    script:
      - >-
        python -m prodigy ner.openai.correct openai_ner_anem corpus/anem-test_texts.jsonl
        --prompt-path openai/templates/${vars.prompt_template}
        --labels Cell,Organism_substance,Pathological_formation,Multitissue_structure,Organism_subdivision,Organ,Cellular_component,Anatomical_system,Tissue,Developing_anatomical_structure,Immaterial_anatomical_entity 
        -F ./openai/recipes/ner.py
    deps:
      - corpus/anem-test_texts.jsonl

  - name: "openai-evaluate"
    help: "Evaluate zero-shot GPT-3 predictions"
    script:
      - >-
        python -m scripts.evaluate_gpt 
        openai/zeroshot_preds.jsonl corpus/anem-test.spacy
        --output metrics/scores_gpt.json
    deps:
      - openai/zeroshot_preds.jsonl
      - corpus/anem-test.spacy
    outputs:
      - metrics/scores_gpt.json

  - name: "train-curve"
    help: "Train a model at varying portions of the training data"
    script:
      # Create JSONL files to hydrate prodigy dataset into
      - python -m scripts.convert_to_jsonl corpus/anem-train.spacy corpus/anem-train.jsonl
      - python -m scripts.convert_to_jsonl corpus/anem-dev.spacy corpus/anem-dev.jsonl
      # Hydrate the prodigy dataset
      - python -m prodigy db-in ${vars.prodigy_dataset} corpus/anem-train.jsonl
      - python -m prodigy db-in ${vars.prodigy_dataset}_eval corpus/anem-dev.jsonl
      # Run train-curve command
      - >-
        python -m prodigy train-curve
        --ner ${vars.prodigy_dataset},eval:${vars.prodigy_dataset}_eval
        --config configs/${vars.config}
        --gpu-id ${vars.gpu_id}
        --show-plot
    deps:
      - corpus/anem-train.spacy
      - corpus/anem-dev.spacy
    outputs:
      - corpus/anem-train.jsonl
      - corpus/anem-dev.jsonl

  - name: "clean-datasets"
    help: "Drop the Prodigy dataset that was automatically created during the train-curve command"
    script:
      - python -m prodigy drop ${vars.prodigy_dataset}
      - python -m prodigy drop ${vars.prodigy_dataset}_eval
