title: "Training a spaCy Coref Model"
description: |
  This project trains a coreference model for spaCy using OntoNotes.

  Before using this project:

  1. install spaCy with GPU support - see the [install widget](https://spacy.io/usage)
  2. run `pip install -r requirements.txt`
  3. modify `project.yml` to set your GPU ID and OntoNotes path

  After that you can just run `spacy project run all`.

  Note that during training you will probably see a warning like `Token indices
  sequence length is longer than ...`. This is a rare condition that
  `spacy-transformers` handles internally, and it's safe to ignore if it
  happens occasionally. For more details see [this
  thread](https://github.com/explosion/spaCy/discussions/9277#discussioncomment-1374226).

  ## Using the Trained Pipeline

  After you've trained the pipeline, you can load and use it like this:

  ```
  import spacy
  nlp = spacy.load("training/coref")

  doc = nlp("John Smith called from New York, he says it's raining in the city.")
  # check the word clusters
  print("=== word clusters ===")
  word_clusters = [val for key, val in doc.spans.items() if key.startswith("coref_head")]
  for cluster in word_clusters:
      print(cluster)
  # check the expanded clusters
  print("=== full clusters ===")
  full_clusters = [val for key, val in doc.spans.items() if key.startswith("coref_cluster")]
  for cluster in full_clusters:
      print(cluster)
  ```

  The prefixes used here are a user setting, so you can customize them for your
  own pipelines.

spacy_version: ">=3.3.0,<4.0.0"

vars:
  # XXX Change to your actual GPU ID
  gpu_id: 0
  # XXX fill with your local path to OntoNotes
  ontonotes: /home/USER/ontonotes5/data
  max_epochs: 20
  test_file: assets/litbank/95_the_prisoner_of_zenda_brat.conll

directories: ["assets", "corpus", "scripts", "configs", "training"]

assets:
  - dest: assets/
    git:
      repo: "https://github.com/explosion/conll-2012"
      branch: "main"
      path: ""
    description: "CoNLL-2012 scripts and dehydrated data."
  - dest: ${vars.ontonotes}
    description: "Ensure you have a local copy of OntoNotes: https://catalog.ldc.upenn.edu/LDC2013T19"    
  - dest: assets/litbank
    extra: True
    git:
      repo: "https://github.com/dbamman/litbank"
      branch: "master"
      path: "coref/conll"
    description: "LitBank dataset. Only used for building data for tests."

workflows:
  prep:
    - prep-data
    - preprocess
  train:
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
  ci-test:
    - prep-test-data
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
    - eval

  all:
    # prep
    - prep-data
    - preprocess
    # train
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
    # eval
    - eval

commands:
  - name: "prep-data"
    help: "Rehydrate the data using OntoNotes"
    script: 
      - bash scripts/prep_data.sh "${vars.ontonotes}"
    deps:
      - assets/conll-2012-development.v4.tar.gz
      - assets/conll-2012-test-key.tar.gz
      - assets/conll-2012-test-official.v9.tar.gz
      - assets/conll-2012-train.v4.tar.gz
      - assets/conll-2012/v3/scripts/skeleton2conll.sh
      - ${vars.ontonotes}
    outputs: 
      - assets/train.gold.conll
      - assets/dev.gold.conll
      - assets/test.gold.conll

  - name: "prep-test-data"
    # Note this writes to the same paths as real data in order to re-use the actions.
    help: "Prepare minimal dataset for CI testing. Note this will overwrite train/dev/test data!"
    deps:
      - ${vars.test_file}
      - scripts/preprocess.py
    script:
      - python scripts/preprocess.py ${vars.test_file} corpus/train.spacy
      - python scripts/preprocess.py ${vars.test_file} corpus/dev.spacy
      - python scripts/preprocess.py ${vars.test_file} corpus/test.spacy
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy

  - name: "preprocess"
    help: "Convert the data to spaCy's format"
    deps:
      - assets/train.gold.conll
      - assets/dev.gold.conll
      - assets/test.gold.conll
      - scripts/preprocess.py
    script:
      - python scripts/preprocess.py assets/train.gold.conll corpus/train.spacy
      - python scripts/preprocess.py assets/dev.gold.conll corpus/dev.spacy
      - python scripts/preprocess.py assets/test.gold.conll corpus/test.spacy
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy

  - name: "train-cluster"
    help: "Train the clustering component"
    script: 
      - "python -m spacy train configs/cluster.cfg -g ${vars.gpu_id} --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy -o training/cluster --training.max_epochs ${vars.max_epochs}"
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy
      - configs/cluster.cfg
    outputs:
      - training/cluster/model-best
  
  - name: "prep-span-data"
    help: "Prepare data for the span resolver component."
    script:
      - python scripts/prep_span_data.py --model-path training/cluster/model-best/ --gpu ${vars.gpu_id} --input-path corpus/train.spacy --output-path corpus/spans.train.spacy --head-prefix coref_head_clusters --span-prefix coref_clusters
      - python scripts/prep_span_data.py --model-path training/cluster/model-best/ --gpu ${vars.gpu_id} --input-path corpus/dev.spacy --output-path corpus/spans.dev.spacy --head-prefix coref_head_clusters --span-prefix coref_clusters
    deps:
      - scripts/prep_span_data.py
      - corpus/train.spacy
      - corpus/dev.spacy
      - training/cluster/model-best
    outputs:
      - corpus/spans.train.spacy
      - corpus/spans.dev.spacy
  
  - name: "train-span-resolver"
    help: "Train the span resolver component."
    script:
      - spacy train configs/span.cfg -c scripts/custom_functions.py -g ${vars.gpu_id} --paths.train corpus/spans.train.spacy --paths.dev corpus/spans.dev.spacy --training.max_epochs ${vars.max_epochs} --paths.transformer_source training/cluster/model-best -o training/span_resolver
    deps:
      - corpus/spans.train.spacy
      - corpus/spans.dev.spacy
      - configs/span.cfg
      - training/cluster/model-best
    outputs:
      - training/span_resolver/model-best

  - name: "assemble"
    help: "Assemble all parts into a complete coref pipeline."
    script:
      - spacy assemble configs/coref.cfg training/coref
    deps:
      - training/cluster/model-best
      - training/span_resolver/model-best
      - configs/coref.cfg

  - name: "eval"
    help: "Evaluate model on the test set."
    script:
      - python scripts/run_eval.py training/coref corpus/test.spacy
