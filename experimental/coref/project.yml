title: "Training a spaCy Coref Model"
description: |
  This project trains a coreference model for spaCy using OntoNotes.

  Before using this project:

  1. install spaCy with GPU support - see the [install widget](https://spacy.io/usage)
  2. run `pip install -r requirements.txt`
  3. modify `project.yml` to set your GPU ID and OntoNotes path (see [Data Preparation](#data-preparation) for details)

  After that you can just run `spacy project run all`.

  Note that during training you will probably see a warning like `Token indices
  sequence length is longer than ...`. This is a rare condition that
  `spacy-transformers` handles internally, and it's safe to ignore if it
  happens occasionally. For more details see [this
  thread](https://github.com/explosion/spaCy/discussions/9277#discussioncomment-1374226).

  ## Data Preparation

  To use this project you need a copy of [OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19).

  If you have OntoNotes and have not worked with the CoNLL 2012 coreference annotations before, set `vars.ontonotes` in `project.yml` to the local path to OntoNotes. The top level directory should contain directories named `arabic`, `chinese`, `english`, and `ontology`. Then run the following command to prepare the coreference data:

  ```
  spacy project run prep-conll-data
  ```

  After that you can execute `spacy project run all`.

  If you already have CoNLL 2012 data prepared and concatenated into one file per split, you can specify the paths to the training, dev, and test files directly in `project.yml`, see the `vars` section. After doing so you can run `spacy project run all`.

  ## Using the Trained Pipeline

  After training the pipeline (or downloading a pretrained version), you can load and use it like this:

  ```
  import spacy
  nlp = spacy.load("training/coref")

  doc = nlp("John Smith called from New York, he says it's raining in the city.")
  # check the word clusters
  print("=== word clusters ===")
  word_clusters = [val for key, val in doc.spans.items() if key.startswith("coref_head")]
  for cluster in word_clusters:
      print(cluster)
  # check the expanded clusters
  print("=== full clusters ===")
  full_clusters = [val for key, val in doc.spans.items() if key.startswith("coref_cluster")]
  for cluster in full_clusters:
      print(cluster)
  ```

  The prefixes used here are a user setting, so you can customize them for your
  own pipelines.

spacy_version: ">=3.3.0,<4.0.0"

vars:
  # XXX Change to your actual GPU ID
  gpu_id: 0
  # XXX fill with your local path to OntoNotes
  ontonotes: /home/USER/ontonotes5/data
  max_epochs: 20
  # File to use for data in ci testing
  test_file: assets/litbank/95_the_prisoner_of_zenda_brat.conll
  # Changed in testing to use CPU configs
  config_dir: configs/

  # conll training data split files
  train_conll: assets/train.gold.conll
  dev_conll: assets/dev.gold.conll
  test_conll: assets/test.gold.conll

directories: ["assets", "corpus", "scripts", "configs", "training"]

assets:
  - dest: assets/
    extra: True
    git:
      repo: "https://github.com/explosion/conll-2012"
      branch: "main"
      path: ""
    description: "CoNLL-2012 scripts and dehydrated data, used for preprocessing OntoNotes."
  - dest: assets/litbank
    extra: True
    git:
      repo: "https://github.com/dbamman/litbank"
      branch: "master"
      path: "coref/conll"
    description: "LitBank dataset. Only used for building data for tests."

workflows:
  prep:
    - preprocess
  train:
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
  ci-test:
    - prep-test-data
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
    - eval

  all:
    # prep
    - preprocess
    # train
    - train-cluster
    - prep-span-data
    - train-span-resolver
    - assemble
    # eval
    - eval

commands:
  - name: "prep-ontonotes-data"
    help: "Rehydrate the data using OntoNotes"
    script: 
      - bash scripts/prep_data.sh "${vars.ontonotes}"
    deps:
      - assets/conll-2012-development.v4.tar.gz
      - assets/conll-2012-test-key.tar.gz
      - assets/conll-2012-test-official.v9.tar.gz
      - assets/conll-2012-train.v4.tar.gz
      - assets/conll-2012/v3/scripts/skeleton2conll.sh
      - ${vars.ontonotes}
    outputs: 
      - assets/train.gold.conll
      - assets/dev.gold.conll
      - assets/test.gold.conll

  - name: "prep-test-data"
    # Note this writes to the same paths as real data in order to re-use the actions.
    help: "Prepare minimal dataset for CI testing. Note this will overwrite train/dev/test data!"
    deps:
      - ${vars.test_file}
      - scripts/preprocess.py
    script:
      - python scripts/preprocess.py ${vars.test_file} corpus/train.spacy
      - python scripts/preprocess.py ${vars.test_file} corpus/dev.spacy
      - python scripts/preprocess.py ${vars.test_file} corpus/test.spacy
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy

  - name: "preprocess"
    help: "Convert the data to spaCy's format"
    deps:
      - ${vars.train_conll}
      - ${vars.dev_conll}
      - ${vars.test_conll}
      - scripts/preprocess.py
    script:
      - python scripts/preprocess.py ${vars.train_conll} corpus/train.spacy
      - python scripts/preprocess.py ${vars.dev_conll} corpus/dev.spacy
      - python scripts/preprocess.py ${vars.test_conll} corpus/test.spacy
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy

  - name: "train-cluster"
    help: "Train the clustering component"
    script: 
      - "python -m spacy train ${vars.config_dir}/cluster.cfg -g ${vars.gpu_id} --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy -o training/cluster --training.max_epochs ${vars.max_epochs}"
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy
      - ${vars.config_dir}/cluster.cfg
    outputs:
      - training/cluster/model-best
  
  - name: "prep-span-data"
    help: "Prepare data for the span resolver component."
    script:
      - python scripts/prep_span_data.py --use-gold-heads --model-path training/cluster/model-best/ --gpu ${vars.gpu_id} --input-path corpus/train.spacy --output-path corpus/spans.train.spacy --head-prefix coref_head_clusters --span-prefix coref_clusters
      - python scripts/prep_span_data.py --use-gold-heads --model-path training/cluster/model-best/ --gpu ${vars.gpu_id} --input-path corpus/dev.spacy --output-path corpus/spans.dev.spacy --head-prefix coref_head_clusters --span-prefix coref_clusters
    deps:
      - scripts/prep_span_data.py
      - corpus/train.spacy
      - corpus/dev.spacy
      - training/cluster/model-best
    outputs:
      - corpus/spans.train.spacy
      - corpus/spans.dev.spacy
  
  - name: "train-span-resolver"
    help: "Train the span resolver component."
    script:
      - spacy train ${vars.config_dir}/span.cfg -c scripts/custom_functions.py -g ${vars.gpu_id} --paths.train corpus/spans.train.spacy --paths.dev corpus/spans.dev.spacy --training.max_epochs ${vars.max_epochs} --paths.transformer_source training/cluster/model-best -o training/span_resolver
    deps:
      - corpus/spans.train.spacy
      - corpus/spans.dev.spacy
      - ${vars.config_dir}/span.cfg
      - training/cluster/model-best
    outputs:
      - training/span_resolver/model-best

  - name: "assemble"
    help: "Assemble all parts into a complete coref pipeline."
    script:
      - spacy assemble ${vars.config_dir}/coref.cfg training/coref
    deps:
      - training/cluster/model-best
      - training/span_resolver/model-best
      - ${vars.config_dir}/coref.cfg

  - name: "eval"
    help: "Evaluate model on the test set."
    script:
      - python scripts/run_eval.py --model training/coref --test-data corpus/test.spacy --gpu ${vars.gpu_id}
