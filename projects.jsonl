{"shortname": "integrations/streamlit", "title": "Streamlit integration", "description": "[Streamlit](https://streamlit.io) is a Python framework for building interactive data apps. The [`spacy-streamlit`](https://github.com/explosion/spacy-streamlit) package helps you integrate spaCy visualizations into your Streamlit apps and quickly spin up demos to explore your pipelines interactively. It includes a full embedded visualizer, as well as individual components. If you're training your own pipelines, you can integrate the `visualize` command into your `project.yml` and pass in the path to your exported pipeline to visualize it. See the tutorial project templates for examples."}
{"shortname": "integrations/fastapi", "title": "FastAPI integration", "description": "Use [FastAPI](https://fastapi.tiangolo.com/) to serve your spaCy models and host modern REST APIs. To install the dependencies and start the server, you can run `spacy project run start`. To explore the REST API interactively, navigate to `http://127.0.0.1:5000/docs` in your browser. See the examples for how to query the API using Python or JavaScript."}
{"shortname": "integrations/prodigy", "title": "Prodigy annotation tool integration", "description": "This project shows how to integrate the [Prodigy](https://prodi.gy) annotation tool (requires **v1.11+**) into your spaCy project template to automatically **export annotations** you've created and **train your model** on the collected data. Note that in order to run this template, you'll need to install Prodigy separately into your environment. For details on how the data was created, check out this [project template](https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands) and [blog post](https://explosion.ai/blog/sense2vec-reloaded#annotation).\n> \u26a0\ufe0f **Important note:** The example in this project uses a separate step `db-in` to export the example annotations into your database, so you can easily run it end-to-end. In your own workflows, you can leave this out and access the given dataset you've annotated directly."}
{"shortname": "integrations/huggingface_hub", "title": "Hugging Face Hub integration", "description": "With [Hugging Face Hub](https://https://huggingface.co/), you can easily share any trained pipeline with the community. The Hugging Face Hub offers:\n\n- Free model hosting.\n- Built-in file versioning, even with very large files, thanks to a git-based approach.\n- In-browser widgets to play with the uploaded models.\n\nThis uses [`spacy-huggingface-hub`](https://github.com/explosion/spacy-huggingface-hub) to push a packaged pipeline to the Hugging Face Hub, including the `whl` file. This enables using `pip install`ing a pipeline directly from the Hugging Face Hub.\n"}
{"shortname": "integrations/wandb", "title": "Weights & Biases integration", "description": "Use [Weights & Biases](https://www.wandb.com/) for logging of training experiments. This project template uses the IMDB Movie Review Dataset and includes two workflows: `log` for training a simple text classification model and logging the results to Weights & Biases (works out-of-the-box and only requires the `[training.logger]` to be set in the config) and `parameter-search` for running a hyperparameter search using [Weights & Biases Sweeps](https://docs.wandb.ai/guides/sweeps), running the experiments and logging the results."}
{"shortname": "integrations/ray", "title": "Ray integration", "description": "Use [Ray](https://ray.io) and the [`spacy-ray`](https://github.com/explosion/spacy-ray) extension package for parallel and distributed training. To configure the number of workers, you can change the `n_workers` variable in the `project.yml`."}
{"shortname": "benchmarks/parsing_penn_treebank", "title": "Dependency Parsing (Penn Treebank)", "description": ""}
{"shortname": "benchmarks/nel", "title": "NEL Benchmark", "description": "Pipeline for benchmarking NEL approaches (incl. candidate generation and entity disambiguation)."}
{"shortname": "benchmarks/textcat_architectures", "title": "Textcat performance benchmarks", "description": "Benchmarking different textcat architectures on different datasets."}
{"shortname": "benchmarks/speed", "title": "Project for speed benchmarking of various pretrained models of different NLP libraries.", "description": "This project runs various models on unannotated text, to measure the average speed in words per second (WPS). Note that a fair comparison should also take into account the type of annotations produced by each model, and the accuracy scores of the various pretrained NLP tasks. This example project only addresses the speed issue, but can be extended to perform more detailed comparisons on any data."}
{"shortname": "benchmarks/ner_conll03", "title": "Named Entity Recognition (CoNLL-2003)", "description": ""}
{"shortname": "benchmarks/healthsea_spancat", "title": "Healthsea-Spancat", "description": "This spaCy project uses the Healthsea dataset to compare the performance between the Spancat and NER architecture."}
{"shortname": "benchmarks/ud_benchmark", "title": "Universal Dependencies v2.5 Benchmarks", "description": "This project template lets you train a spaCy pipeline on any [Universal Dependencies](https://universaldependencies.org/) corpus (v2.5) for benchmarking purposes. The pipeline includes an experimental trainable tokenizer, an experimental edit tree lemmatizer, and the standard spaCy tagger, morphologizer and dependency parser components. The CoNLL 2018 evaluation script is used to evaluate the pipeline. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `ud_treebank` and `spacy_lang` settings in the config. Use `xx` (multi-language) for `spacy_lang` if a particular language is not supported by spaCy. The tokenizer in particular is only intended for use in this generic benchmarking setup. It is not optimized for speed and it does not perform particularly well for languages without space-separated tokens. In production, custom rules for spaCy's rule-based tokenizer or a language-specific word segmenter such as jieba for Chinese or sudachipy for Japanese would be recommended instead."}
{"shortname": "nel/wikid", "title": "wikid", "description": "[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/32/main.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=32)\n[![spaCy](https://img.shields.io/static/v1?label=made%20with%20%E2%9D%A4%20and&message=spaCy&color=09a3d5&style=flat-square)](https://spacy.io)\n<br/>\n_No REST for the `wikid`_ :jack_o_lantern: - generate a SQLite database and a spaCy `KnowledgeBase` from Wikipedia & \nWikidata dumps. `wikid` was designed with the use case of named entity linking (NEL) with spaCy in mind.\n<br/>\nNote this repository is still in an experimental stage, so the public API might change at any time. \n"}
{"shortname": "experimental/ner_spancat", "title": "Example SpanCategorizer project using Indonesian NER", "description": "The SpanCategorizer is a component in **spaCy v3.1+** for assigning labels to contiguous spans of text proposed by a customizable suggester function. Unlike spaCy's EntityRecognizer component, the SpanCategorizer can recognize nested or overlapping spans. It also doesn't rely as heavily on consistent starting and ending words, so it may be a better fit for non-NER span labelling tasks. You do have to write a function that proposes your candidate spans, however. If your spans are often short, you could propose all spans under a certain size. You could also use syntactic constituents such as noun phrases or noun chunks, or matcher rules."}
{"shortname": "experimental/coref", "title": "Training a spaCy Coref Model", "description": "This project trains a coreference model for spaCy using OntoNotes.\n\nBefore using this project:\n\n1. install spaCy with GPU support - see the [install widget](https://spacy.io/usage)\n2. run `pip install -r requirements.txt`\n3. modify `project.yml` to set your GPU ID and OntoNotes path (see [Data Preparation](#data-preparation) for details)\n\nAfter that you can just run `spacy project run all`.\n\nNote that during training you will probably see a warning like `Token indices\nsequence length is longer than ...`. This is a rare condition that\n`spacy-transformers` handles internally, and it's safe to ignore if it\nhappens occasionally. For more details see [this\nthread](https://github.com/explosion/spaCy/discussions/9277#discussioncomment-1374226).\n\n## Data Preparation\n\nTo use this project you need a copy of [OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19).\n\nIf you have OntoNotes and have not worked with the CoNLL 2012 coreference annotations before, set `vars.ontonotes` in `project.yml` to the local path to OntoNotes. The top level directory should contain directories named `arabic`, `chinese`, `english`, and `ontology`. Then run the following command to prepare the coreference data:\n\n```\nspacy project run prep-conll-data\n```\n\nAfter that you can execute `spacy project run all`.\n\nIf you already have CoNLL 2012 data prepared and concatenated into one file per split, you can specify the paths to the training, dev, and test files directly in `project.yml`, see the `vars` section. After doing so you can run `spacy project run all`.\n\n## Using the Trained Pipeline\n\nAfter training the pipeline (or downloading a pretrained version), you can load and use it like this:\n\n```\nimport spacy\nnlp = spacy.load(\"training/coref\")\n\ndoc = nlp(\"John Smith called from New York, he says it's raining in the city.\")\n# check the word clusters\nprint(\"=== word clusters ===\")\nword_clusters = [val for key, val in doc.spans.items() if key.startswith(\"coref_head\")]\nfor cluster in word_clusters:\n    print(cluster)\n# check the expanded clusters\nprint(\"=== full clusters ===\")\nfull_clusters = [val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")]\nfor cluster in full_clusters:\n    print(cluster)\n```\n\nThe prefixes used here are a user setting, so you can customize them for your\nown pipelines.\n"}
{"shortname": "experimental/ner_spancat_compare", "title": "Comparing SpanCat and NER using a corpus of biomedical literature (GENIA)", "description": "This project demonstrates how spaCy's Span Categorization (SpanCat) and\nNamed-Entity Recognition (NER) perform on different types of entities. Here, we used\na dataset of biomedical literature containing both overlapping and non-overlapping spans.\n\n### About the dataset\n\n[GENIA](http://www.geniaproject.org/genia-corpus) is a dataset containing\nbiomedical literature from 1,999 Medline abstracts. It contains a collection\nof overlapping and hierarchical spans. To make parsing easier, we will be\nusing the [pre-constructed IOB\ntags](https://github.com/thecharm/boundary-aware-nested-ner/blob/master/Our_boundary-aware_model/data/genia)\nfrom the [Boundary Aware Nested NER\npaper](https://aclanthology.org/D19-1034/)\n[repository](https://github.com/thecharm/boundary-aware-nested-ner/). Running `debug data` gives us the\nfollowing span characteristics (**SD** = Span Distinctiveness, **BD** = Boundary Distinctiveness):\n\n| Span Type | Span Length | SD                   | BD                       |\n|-----------|-------------|----------------------|--------------------------|\n| DNA       | 2.81        | 1.45                 | 0.80                     |\n| protein   | 2.19        | 1.19                 | 0.57                     |\n| cell_type | 2.09        | 2.35                 | 1.05                     |\n| cell_line | 3.29        | 1.91                 | 1.04                     |\n| RNA       | 2.73        | 2.68                 | 1.28                     |\n\nThe table above shows the average span length for each span type, and their\ncorresponding distinctiveness characteristics. The latter is computed using\nthe KL-divergence of the span's token distribution with respect to the overall\ncorpus's. The higher the number is, the more distinct the tokens are compared to the\nrest of the corpus.\n\nThese characteristics can give us a good intuition as to how well the SpanCat\nmodel identifies the correct spans. In the case of GENIA, the entities\nthemselves tend to be technical terms, which makes it more distinct and easier\nto classify. Again, we measure distinctiveness not only within the entities\nthemselves (SD), but also in its boundaries (BD).\n\nHere's some example data:\n\n![](./images/sample_00.png)\n![](./images/sample_01.png)\n\n\n### Experiments\n\nGiven what we know from the dataset, we will create the following pipelines:\n\n| Pipeline | Description                                                                                                                             | Workflow Name |\n|----------|-----------------------------------------------------------------------------------------------------------------------------------------|---------------|\n| SpanCat  | Pure Span Categorization for all types of entities. Serves as illustration to demonstrate suggester functions and as comparison to NER. | `spancat` |\n| NER      | Named-Entity Recognition for all types of entities. Serves as illustration to compare with the pure SpanCat implementation       | `ner`         |\n\n\n#### SpanCat Results\n\nBelow are the results for SpanCat. It seems that overall, span categorization\nis biased towards precision. This means that a large number of the suggested\nspans belong to the correct class. We can always tune how precise we want it\nto be: make the suggester lenient and we might get a lot of irrelevant hits,\nmake it strict and we miss might out on some true positives.\n\n|           | Precision   | Recall   | F-score   |\n|-----------|-------------|----------|-----------|\n| DNA       |    0.70     | 0.36     |  0.47     |\n| protein   |    0.77     | 0.52     |  0.62     |\n| cell_line |    0.77     | 0.30     |  0.44     |\n| cell_type |    0.76     | 0.62     |  0.68     |\n| RNA       |    0.77     | 0.25     |  0.38     |\n| **Overall**| 0.76       | 0.47     |  0.58     |\n\n#### NER Results\n\nNER performs well against SpanCat for all entity types, but note that this\nprocess entails training five (5) models per entity type. This might work if\nyou have a small number of entities, but can be computationally heavy if you\nhave a lot.\n\n|           |   Precision | Recall |   F-score |\n|-----------|-------------|--------|-----------|\n| DNA       |    0.74     | 0.63   |  0.68     |\n| protein   |    0.76     | 0.72   |  0.74     |\n| cell_line |    0.74     | 0.57   |  0.64     |\n| cell_type |    0.78     | 0.72   |  0.75     |\n| RNA       |    0.86     | 0.65   |  0.74     |\n\nSince we have five (5) separate models in NER, what we can do afterwards is\ncombine them into a single `Doc` that transfers `doc.ents` to `doc.spans`. Since\nthe tokens are the same, we don't need to worry about misalignments and the like.\n"}
{"shortname": "experimental/ner_wikiner_speedster", "title": "Named Entity Recognition (WikiNER) accelerated using speedster", "description": "This project shows how `speedster` can accelerate spaCy's WikiNER pipeline.\n\n[Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) is an open-source tool designed to accelerate AI inference of deep learning models in a few lines of code. Within the WikiNER pipeline, `speedster` optimizes BERT to achieve the maximum acceleration physically possible on the hardware used.\n\n`Speedster` is built on top of [Nebullvm](https://github.com/nebuly-ai/nebullvm), an open-source framework for building AI-optimization tools.\n\nFurther info on the WikiNER pipeline can be found in [this section](https://github.com/explosion/projects/tree/v3/pipelines/ner_wikiner)."}
{"shortname": "pipelines/parser_intent_demo", "title": "Demo Intent Parser (Dependency Parser)", "description": "A minimal demo parser project for spaCy v3 adapted from the spaCy v2 [`train_intent_parser.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_intent_parser.py) example script."}
{"shortname": "pipelines/ner_demo_replace", "title": "Demo replacing an NER component in a pretrained pipeline", "description": "A minimal demo NER project that replaces the NER component in an existing pretrained pipeline. All other pipeline components are preserved and frozen during training."}
{"shortname": "pipelines/floret_wiki_oscar_vectors", "title": "Train floret vectors from Wikipedia and OSCAR", "description": "This project downloads, extracts and preprocesses texts from Wikipedia and\nOSCAR and trains vectors with [floret](https://github.com/explosion/floret).\n\nBy default, the project trains floret vectors for Macedonian.\n\nPrerequisites:\n- a large amount of hard drive space\n- a workstation with a good CPU, or a lot of patience\n\nFor Macedonian, you'll need ~5GB in `/scratch` and ~1GB in `vectors/`.\n\nAdjust the variables `n_process` and `vector_thread` for your CPU.\n\n## Text Sources\n\n- Wikipedia: https://dumps.wikimedia.org\n- OSCAR 2019: https://oscar-corpus.com/post/oscar-2019/\n\nBy default the full OSCAR 2019 dataset is loaded in streaming mode. Adjust\n`oscar_max_texts` to use a subset of the full dataset, especially for large\nlanguages like English, Spanish, Chinese, Russian, etc. The text lengths are\nnot consistent, but 1M texts may be ~3-5GB.\n\n## wikiextractor\n\nIn order to fix a few bugs and support multiprocessing with spawn, this\nproject installs a fork of [`wikiextractor`\nv3.0.6](https://github.com/attardi/wikiextractor) as wikiextractor v3.0.7a0.\nThe modifications to wikiextractor v3.0.6 are described in [this\ncommit](https://github.com/adrianeboyd/wikiextractor/commit/f8b539d46cd67205884d701c1d5fd18eda84825f).\n\n## wikiextractor\n\nIn order to fix a few bugs and support multiprocessing with spawn, this\nproject installs a fork of [`wikiextractor`\nv3.0.6](https://github.com/attardi/wikiextractor) as wikiextractor v3.0.7a0.\nThe modifications to wikiextractor v3.0.6 are described in [this\ncommit](https://github.com/adrianeboyd/wikiextractor/commit/f8b539d46cd67205884d701c1d5fd18eda84825f).\n\n## floret Parameters\n\n[floret](https://github.com/explosion/floret) has a large number of\nparameters and it's difficult to give advice for all configurations, but the\nparameters described here are the ones that it makes sense to customize for\nany new language and to experiment with initially.\n\nBe aware that if you're using more than one thread, the results of each run\nwith fastText or floret will be slightly different.\n\n### `vector_minn` / `vector_maxn`\n\nThe minimum and maximum character n-gram lengths should be adapted for the\nlanguage and writing system. The n-grams should capture common grammatical\naffixes like English `-ing`, without making the number of n-grams per word\ntoo large. Very short n-grams aren't meaningful and very long n-grams will be\ntoo sparse and won't be useful for cases with misspellings and noise.\n\nA good rule of thumb is that `maxn` should correspond to the length of the\nlongest common affix + `1`, so for many languages with alphabets, `minn\n5`/`maxn 5` can be a good starting point, similar to the defaults in the\n[original fastText vectors](https://fasttext.cc/docs/en/crawl-vectors.html).\n\nFor writing systems where one character corresponds to a syllable, shorter\nn-grams are typically more suitable. For Korean, where each (normalized)\ncharacter is a syllable and most grammatical affixes are 1-2 characters,\n`minn 2`/`maxn 3` seems to perform well.\n\n### `vector_bucket`\n\nThe bucket size is the number of rows in the floret vector table. For\ntagging and parsing, a bucket size of 50k performs well, but larger sizes may\nstill lead to small improvements. For NER, the performance continues to\nimprove for bucket sizes up to at least 200k.\n\nIn a spaCy pipeline package, 50k 300-dim vectors are ~60MB and 200k 300-dim\nvectors are ~230MB.\n\n### `vector_hash_count`\n\nThe recommended hash count is `2`, especially for smaller bucket sizes.\n\nLarger hash counts are slower to train with floret and slightly slower in\ninference in spaCy, but may lead to slightly improved performance, especially\nwith larger bucket sizes.\n\n### `vector_epoch`\n\nYou may want to reduce the number of epochs for larger training input sizes.\n\n### `vector_min_count`\n\nYou may want to increase the minimum word count for larger training input\nsizes.\n\n### `vector_lr`\n\nYou may need to decrease the learning rate for larger training input sizes to\navoid NaN errors, see:\nhttps://fasttext.cc/docs/en/faqs.html#im-encountering-a-nan-why-could-this-be\n\n### `vector_thread`\n\nAdjust the number of threads for your CPU. With a larger number of threads,\nyou may need more epochs to reach the same performance.\n\n## Notes\n\nThe project does not currently clean up any intermediate files so that it's\npossible to resume from any point in the workflow. The overall disk space\ncould be reduced by cleaning up files after each step, keeping only the final\nfloret input text file. floret does require the input file to be on disk\nduring training.\n\nfloret always writes the full `.bin` and `.vec` files after training. These\nmay be 5GB+ each even though the final `.floret` table is much smaller.\n\nImport the floret vectors into a spaCy vectors model with:\n\n```shell\nspacy init vectors mk vectors/mk.floret /path/to/mk_vectors_model --mode floret\n```\n"}
{"shortname": "pipelines/floret_vectors_demo", "title": "Demo floret vectors", "description": "Train floret vectors and load them into a spaCy vectors model."}
{"shortname": "pipelines/ner_wikiner", "title": "Named Entity Recognition (WikiNER)", "description": "Simple example of downloading and converting source data and training a named entity recognition model. The example uses the WikiNER corpus, which was constructed semi-automatically. The main advantage of this corpus is that it's freely available, so the data can be downloaded as a project asset. The WikiNER corpus is distributed in IOB format, a fairly common text encoding for sequence data. The `corpus` subcommand splits the corpus into training, development and testing partitions, and uses `spacy convert` to convert them into spaCy's binary format. You can then edit the config to try out different settings, and trigger training with the `train` subcommand."}
{"shortname": "pipelines/textcat_demo", "title": "Demo Textcat (Text Classification)", "description": "A minimal demo textcat project for spaCy v3. The demo data comes from the [tutorials/textcat_docs_issues](https://github.com/explosion/projects/tree/v3/tutorials/textcat_docs_issues) project."}
{"shortname": "pipelines/ner_demo", "title": "Demo NER in a new pipeline (Named Entity Recognition)", "description": "A minimal demo NER project for spaCy v3 adapted from the spaCy v2 [`train_ner.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_ner.py) example script for creating an NER component in a new pipeline."}
{"shortname": "pipelines/floret_ko_ud_demo", "title": "Demo floret vectors for UD Korean Kaist", "description": "Train floret vectors on OSCAR and compare no vectors, standard vectors, and floret vectors on UD Korean Kaist."}
{"shortname": "pipelines/parser_demo", "title": "Demo Dependency Parser", "description": "A minimal demo parser project for spaCy v3 adapted from the spaCy v2 [`train_parser.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_parser.py) example script."}
{"shortname": "pipelines/spancat_demo", "title": "Demo spancat in a new pipeline (Span Categorization)", "description": "A minimal demo spancat project for spaCy v3"}
{"shortname": "pipelines/tagger_parser_ud", "title": "Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)", "description": "This project template lets you train a part-of-speech tagger, morphologizer, lemmatizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\n"}
{"shortname": "pipelines/tagger_parser_predicted_annotations", "title": "Using Predicted Annotations in Subsequent Components", "description": "This project shows how to use the predictions from one pipeline component as features for a subsequent pipeline component in **spaCy v3.1+**. In this demo, which trains a parser and a tagger on [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT), the `token.dep` attribute from the parser is used as a feature by the tagger. To make the predicted `DEP` available to the tagger during training, `DEP` is added to `[components.tagger.model.tok2vec.embed.attrs]` and `parser` is added to `[training.annotating_components]` in the config. This particular example does not lead to a large difference in performance, but the tagger accuracy improves from to 92.67% to 92.97% with the addition of `DEP`."}
{"shortname": "pipelines/polar_component", "title": "Polar Component", "description": "This example project shows how to implement a simple stateful component to\nscore docs on semantic poles.\n\nThe method here is based on SemAxis from [An et al\n2018](https://arxiv.org/abs/1806.05521). The basic idea is that given a set\nof word vectors and some seed poles, like \"bad-good\", it's possible to\ncalculate reference vectors. The distance of document vectors from those\nreference vectors is like a sentiment or polar score of the document. While\nnot as sophisticated as a trained model, it's easy to test with existing data.\n\nIf you use enough poles, you can use the scores as semantic vectors that can\nmake downstream tasks explainable. This is explored in the SemAxis paper as\nwell as [Mathew et al 2020](https://arxiv.org/abs/2001.09876), \"The Polar\nFramework\". (Incorporating semantic vectors as features in a spaCy model is\nleft as an exercise for the reader.) \n\n**Note:** Because the data is hosted on Kaggle, it can't be automatically\ndownloaded by `spacy project assets`, so you'll have to download it yourself.\nSee [the assets section of this README](#assets) for the link.\n"}
{"shortname": "pipelines/textcat_multilabel_demo", "title": "Demo Multilabel Textcat (Text Classification)", "description": "A minimal demo textcat_multilabel project for spaCy v3."}
{"shortname": "pipelines/ner_demo_update", "title": "Demo updating an NER component in a pretrained pipeline", "description": "A demo NER project that updates the NER component in an existing pretrained pipeline. All other pipeline components are preserved and frozen during training."}
{"shortname": "pipelines/floret_fi_core_demo", "title": "Demo floret vectors for Finnish", "description": "Train floret vectors on OSCAR and compare standard vectors vs. floret vectors on UD Finnish TDT and turku-ner-corpus."}
{"shortname": "tutorials/spancat_food_ingredients", "title": "Span Categorization in Prodigy", "description": "This project shows how to use Prodigy to annotate data for the spancat component"}
{"shortname": "tutorials/textcat_docs_issues", "title": "Predicting whether a GitHub issue is about documentation (Text Classification)", "description": "This project uses [spaCy](https://spacy.io) with annotated data from [Prodigy](https://prodi.gy) to train a **binary text classifier** to predict whether a GitHub issue title is about documentation. The pipeline uses the component `textcat_multilabel` in order to train a binary classifier using only one label, which can be True or False for each document. An equivalent alternative for a binary text classifier would be to use the `textcat` component with two labels, where exactly one of the two labels is True for each document."}
{"shortname": "tutorials/ner_fashion_brands", "title": "Detecting fashion brands in online comments (Named Entity Recognition)", "description": "This project uses [`sense2vec`](https://github.com/explosion/sense2vec) and [Prodigy](https://prodi.gy) to bootstrap an NER model to detect fashion brands in [Reddit comments](https://files.pushshift.io/reddit/comments/). For more details, see [our blog post](https://explosion.ai/blog/sense2vec-reloaded#annotation)."}
{"shortname": "tutorials/textcat_goemotions", "title": "Categorization of emotions in Reddit posts (Text Classification)", "description": "This project uses spaCy to train a text classifier on the [GoEmotions dataset](https://github.com/google-research/google-research/tree/master/goemotions) with options for a pipeline with and without transformer weights. To use the BERT-based config, change the `config` variable in the `project.yml`. \n\n> The goal of this project is to show how to train a spaCy classifier based on a csv file, not to showcase a model that's ready for production. The GoEmotions dataset has known flaws described [here](https://github.com/google-research/google-research/tree/master/goemotions#disclaimer) as well as label errors resulting from [annotator disagreement](https://www.youtube.com/watch?v=khZ5-AN-n2Y).\n"}
{"shortname": "tutorials/base_project", "title": "A base project to use as a starting point", "description": "This project is a skeleton designed to be used as a starting point for your own projects. The project file includes examples of useful options that you can edit to get started.\n"}
{"shortname": "tutorials/ner_pytorch_medical", "title": "Detecting entities in Medical Records with PyTorch", "description": "This project uses the [i2b2 (n2c2) 2011 Challenge Dataset](https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/) to bootstrap a PyTorch NER model to detect entities in Medical Records. It also demonstrates how to anonymize medical records for annotators in [Prodigy](https://prodi.gy)."}
{"shortname": "tutorials/ner_double", "title": "Combining Multiple Trained NER Components", "description": "This project shows you the different ways you can combine multiple trained NER components and their tradeoffs.\n"}
{"shortname": "tutorials/rel_component", "title": "Example project of creating a novel nlp component to do relation extraction from scratch.", "description": "This example project shows how to implement a spaCy component with a custom Machine Learning model, how to train it with and without a transformer, and how to apply it on an evaluation dataset."}
{"shortname": "tutorials/ner_multiple_trials", "title": "Training a named-entity recognition (NER) with multiple trials", "description": "This project demonstrates how to train a spaCy pipeline with multiple trials.\nIt trains a named-entity recognition (NER) model on the WikiNEuRal English\ndataset.  Having multiple trials is useful for experiments, especially if we\nwant to account for variance and *dependency* on a random seed. \n\nUnder the hood, the training script in `scripts/train_with_trials.py`\ngenerates a random seed per trial, and runs the `train` command as usual.  You\ncan find the trained model per trial in `training/trial_{n}/`.\n\n> **Note**\n> Because the WikiNEuRal dataset is large, we're limiting the number of samples in the train\n> and dev corpus to 500 for demonstration purposes. You can adjust this by\n> overriding `vars.limit_samples`, or setting it to `0` to train on the whole\n> training corpus.\n\nAt evaluation, you can pass a directory containing all the models for each\ntrial. This process is demonstrated in `scripts/evaluate_with_trials.py`.\nThis will then result to multiple `metrics/scores.json` files that you can\nsummarize.\n"}
{"shortname": "tutorials/ner_drugs", "title": "Detecting drug names in online comments (Named Entity Recognition)", "description": "This project uses [Prodigy](https://prodi.gy) to bootstrap an NER model to detect drug names in [Reddit comments](https://files.pushshift.io/reddit/comments/)."}
{"shortname": "tutorials/ner_tweets", "title": "Detecting people entities in tweets (Named Entity Recognition)", "description": "This project demonstrates how to improve spaCy's pretrained models by\naugmenting the training data and adapting it to a different domain.\n\n**Weak supervision** is the practice of labelling a dataset using imprecise\nannotators. Instead of labelling them manually by ourselves (which may take\ntime and effort), we can just encode business rules or heuristics to do the\nlabelling for us. \n\nOf course, these rules don't capture all the nuances a human annotator can.\nSo weak supervision's idea is to **pool all these naive annotators\ntogether**, and come up with a unified annotator that can hopefully understand\nthose nuances. The pooling is done via a Hidden Markov Model (HMM). \n\nOnce we have the unified annotator, we re-annotate our entire training dataset,\nand use that for our downstream tasks, which in this case, is finetuning spaCy's\n`en_core_web_lg` model. \n\n### Using `skweak` as our weak supervision framework\n\nIn this project, we will be using `skweak` as our weak supervision framework.\nIt contains primitives to define our own labelling functions, and it's\nwell-integrated to spaCy! These labelling functions can then be defined as\none of the following:\n\n| Annotator Type | What it does                                                                                                                                                                                                                |\n|----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Model          | makes use of existing models trained from other datasets. Here we can also use spaCy's pretrained models like `en_core_web_lg`                                                                                              |\n| Gazetteer      | a list of entities that an annotator can look up from. This will always depend on your use-case.                                                                                                                            |\n| Heuristics     | can be used to encode business rules based on one's domain. You can check for tokens that fall under a specific rule (`TokenConstraintAnnotator`), or look for spans of entities in a sentence (`SpanConstraintAnnotator`). |\n\nThe unified model is then done using a Hidden Markov Model, in this case, via\nthe [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) pacakge. Once we\nhave our final model, we re-annotate the dataset and resume training via\n`spacy train`.\n\n### Weak supervision in practice \n\nFor our dataset, we have a collection of tweets from the [TweetEval\nBenchmark](https://github.com/cardiffnlp/tweeteval). Because tweets differ in\nstructure and form compared to the datasets our original spaCy models were\ntrained on, it makes sense to adapt the domain. Here are some annotators I\ncreated:\n\n| Annotator                                 | Annotator Type | Heuristic                                                                                                                              |\n|-------------------------------------------|----------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| Model-based annotator from en_core_web_lg | Model          | We can reuse spaCy models to bootstrap our annotation process. By doing this, we don't have to start entirely from scratch.            |\n| Broad Twitter Corpus model                | Model          | Because our dataset is made of tweets, it makes sense to bootstrap from a language model trained on the same domain.                   |\n| List of crunchbase personalities          | Gazetteer      | Obtaining a list of tech  personalities is timely, especially due to the context of some tweets.                                       |\n| Proper names annotator                    | Heuristics     | A simple heuristic that checks if two proper names are joined by an infix (e.g. Vincent **van** Gogh, Mark **de** Castro, etc.)        |\n| Full names annotator                      | Heuristics     | A simple heuristic for finding full names. It checks if a first name exists in a list of names, and is also followed by a proper name. |\n\n\nThere are also some annotators that I've tried, but removed due to its\ndetrimental effect to our evaluation scores:\n\n| Annotator                            | Annotator Type | Heuristic                                                                                                                                      |\n|--------------------------------------|----------------|------------------------------------------------------------------------------------------------------------------------------------------------|\n| List of personalities from Wikipedia | Gazetteer      | Ideally, we want to include all person entities from a bigger database. However, it detects \"you\" as a `PERSON`, which affected our Precision. |\n| Name suffix annotator                | Heuristic      | I wanted to capture names with \"iii\", \"IV\", etc., but it gives lower precision due to it detecting roman numerals that aren't part of names.   |\n"}
{"shortname": "tutorials/nel_emerson", "title": "Disambiguation of \"Emerson\" mentions in sentences (Entity Linking)", "description": "**This project was created as part of a [step-by-step video tutorial](https://www.youtube.com/watch?v=8u57WSXVpmw).** It uses [spaCy](https://spacy.io)'s entity linking functionality and [Prodigy](https://prodi.gy) to disambiguate \"Emerson\" mentions in text to unique identifiers from Wikidata. As an example use-case, we consider three different people called Emerson: [an Australian tennis player](https://www.wikidata.org/wiki/Q312545), [an American writer](https://www.wikidata.org/wiki/Q48226), and a [Brazilian footballer](https://www.wikidata.org/wiki/Q215952). [See here](https://github.com/explosion/projects/tree/master/nel-emerson) for the previous scripts for spaCy v2.x."}
{"shortname": "tutorials/parser_low_resource", "title": "Training a POS tagger and dependency parser for a low-resource language", "description": "This project trains a part-of-speech tagger and dependency parser for a\nlow-resource language such as Tagalog. We will be using the\n[TRG](https://universaldependencies.org/treebanks/tl_trg/index.html) and\n[Ugnayan](https://universaldependencies.org/treebanks/tl_ugnayan/index.html)\ntreebanks for this task. Since the number of sentences in each corpus is\nsmall, we'll need to evaluate our model using [10-fold cross\nvalidation](https://universaldependencies.org/release_checklist.html#data-split).\nHow to implement this split will be demonstrated in this project\n(`scripts/kfold.py`). The cross validation results can be seen below.\n\n|         | TOKEN_ACC | POS_ACC | MORPH_ACC | TAG_ACC | DEP_UAS | DEP_LAS |\n|---------|-----------|---------|-----------|---------|---------|---------|\n| TRG     | **1.000**     | **0.843**   | 0.749     | **0.833**   | *80.846**   | **0.554**   |\n| Ugnayan | 0.998     | 0.819   | **0.995**     | 0.810   | 0.667   | 0.409   |\n"}
{"shortname": "tutorials/ner_food_ingredients", "title": "Analyzing how mentions of ingredients change over time (Named Entity Recognition)", "description": "**This project was created as part of a [step-by-step video tutorial](https://www.youtube.com/watch?v=59BKHO_xBPA).** It uses [`sense2vec`](https://github.com/explosion/sense2vec) and [Prodigy](https://prodi.gy) to bootstrap an NER model to detect ingredients [Reddit comments](https://files.pushshift.io/reddit/comments/) and to calculate how mentions change over time. The results were then used to create a [bar chart race visualization](https://public.flourish.studio/visualisation/1532208/) of selected ingredients."}
