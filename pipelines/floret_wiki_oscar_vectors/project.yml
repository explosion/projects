title: "Train floret vectors from Wikipedia and OSCAR"
description: |
  This project downloads, extracts and preprocesses texts from Wikipedia and
  OSCAR and trains vectors with [floret](https://github.com/explosion/floret).

  By default, the project trains floret vectors for Macedonian.

vars:
  name: "vectors"
  lang: "mk"
  n_process: 16
  # The defaults assume that you have a large hard drive mounted under /scratch
  downloaded_dir: "/scratch/vectors/downloaded"
  extracted_dir: "/scratch/vectors/extracted"
  tokenized_dir: "/scratch/vectors/tokenized"
  wikipedia_version: "latest"
  oscar_dataset: "oscar"
  oscar_dataset_subset: "unshuffled_deduplicated_${vars.lang}"
  oscar_dataset_split: "train"
  # Limit for large languages like English, Spanish, Chinese, Russian
  # Check the size of the raw corpus (under the column "Size deduplicated"):
  # https://oscar-corpus.com/post/oscar-2019/
  oscar_max_texts: -1
  vector_input_dir: "/scratch/vectors/input"
  vector_model: "cbow"
  # For languages with alphabets: minn/maxn 4/5 or 5/5 is a good starting point.
  vector_minn: 5
  vector_maxn: 5
  vector_epoch: 5
  vector_dim: 300
  vector_neg: 10
  vector_bucket: 50000
  vector_min_count: 20
  vector_hash_count: 2
  vector_thread: 16
  vector_lr: 0.05

directories: ["vectors"]

assets:
  - dest: "${vars.downloaded_dir}/wikipedia/${vars.lang}wiki-${vars.wikipedia_version}-pages-articles.xml.bz2"
    url: "https://dumps.wikimedia.org/${vars.lang}wiki/${vars.wikipedia_version}/${vars.lang}wiki-${vars.wikipedia_version}-pages-articles.xml.bz2"

workflows:
  all:
    - extract-wikipedia
    - tokenize-wikipedia
    - tokenize-oscar
    - create-input
    - train-floret-vectors

commands:
  - name: "extract-wikipedia"
    help: "Convert Wikipedia XML to JSONL with wikiextractor"
    script:
      - >-
        python -m wikiextractor.WikiExtractor
        --json --no-templates -b 1000G -q
        --processes ${vars.n_process}
        ${vars.downloaded_dir}/wikipedia/${vars.lang}wiki-${vars.wikipedia_version}-pages-articles.xml.bz2
        -o ${vars.extracted_dir}/${vars.lang}_wiki_${vars.wikipedia_version}/
    outputs:
      - "${vars.extracted_dir}/${vars.lang}_wiki_${vars.wikipedia_version}/AA/wiki_00"

  - name: "tokenize-wikipedia"
    help: "Tokenize and sentencize Wikipedia"
    script:
      - >-
        python scripts/tokenize_resource.py ${vars.lang}
        ${vars.tokenized_dir}/${vars.lang}_wiki_${vars.wikipedia_version}.txt
        --input-jsonl ${vars.extracted_dir}/${vars.lang}_wiki_${vars.wikipedia_version}/AA/wiki_00
        --n-process ${vars.n_process}
    deps:
      - "scripts/tokenize_resource.py"
      - "${vars.extracted_dir}/${vars.lang}_wiki_${vars.wikipedia_version}/AA/wiki_00"
    outputs:
      - "${vars.tokenized_dir}/${vars.lang}_wiki_${vars.wikipedia_version}.txt"

  - name: "tokenize-oscar"
    help: "Tokenize and sentencize OSCAR dataset"
    script:
      - >-
        python scripts/tokenize_resource.py ${vars.lang}
        ${vars.tokenized_dir}/${vars.lang}_oscar_${vars.oscar_dataset_subset}.txt
        --input-dataset ${vars.oscar_dataset}
        --dataset-subset ${vars.oscar_dataset_subset}
        --dataset-split ${vars.oscar_dataset_split}
        --n-process=${vars.n_process}
        --max-texts=${vars.oscar_max_texts}
    deps:
      - "scripts/tokenize_resource.py"
    outputs:
      - "${vars.tokenized_dir}/${vars.lang}_oscar_${vars.oscar_dataset_subset}.txt"

  - name: "create-input"
    help: "Concatenate tokenized input texts"
    script:
      - >-
        python scripts/concat_files.py 
        --input-file ${vars.tokenized_dir}/${vars.lang}_wiki_${vars.wikipedia_version}.txt
        --input-file ${vars.tokenized_dir}/${vars.lang}_oscar_${vars.oscar_dataset_subset}.txt
        ${vars.vector_input_dir}/${vars.lang}.txt
    deps:
      - "scripts/concat_files.py"
      - "${vars.tokenized_dir}/${vars.lang}_wiki_${vars.wikipedia_version}.txt"
      - "${vars.tokenized_dir}/${vars.lang}_oscar_${vars.oscar_dataset_subset}.txt"
    outputs:
      - "${vars.vector_input_dir}/${vars.lang}.txt"

  - name: "train-floret-vectors"
    help: "Train floret vectors"
    script:
      - >-
        python scripts/train_floret.py
        --mode floret
        --model ${vars.vector_model}
        --dim ${vars.vector_dim}
        --mincount ${vars.vector_min_count}
        --minn ${vars.vector_minn}
        --maxn ${vars.vector_maxn}
        --neg ${vars.vector_neg}
        --epoch ${vars.vector_epoch}
        --hashcount ${vars.vector_hash_count}
        --bucket ${vars.vector_bucket}
        --thread ${vars.vector_thread}
        --lr ${vars.vector_lr}
        ${vars.vector_input_dir}/${vars.lang}.txt
        vectors/${vars.lang}
    deps:
      - "scripts/train_floret.py"
      - "${vars.vector_input_dir}/${vars.lang}.txt"
    outputs:
      - "vectors/${vars.lang}.floret"

  - name: "train-fasttext-vectors"
    help: "Train fastText vectors"
    script:
      - >-
        python scripts/train_floret.py
        --mode fasttext
        --model ${vars.vector_model}
        --dim ${vars.vector_dim}
        --mincount ${vars.vector_min_count}
        --minn ${vars.vector_minn}
        --maxn ${vars.vector_maxn}
        --neg ${vars.vector_neg}
        --epoch ${vars.vector_epoch}
        --thread ${vars.vector_thread}
        --lr ${vars.vector_lr}
        --bucket 2000000
        ${vars.vector_input_dir}/${vars.lang}.txt
        vectors/${vars.lang}.fasttext
    deps:
      - "scripts/train_floret.py"
      - "${vars.vector_input_dir}/${vars.lang}.txt"
    outputs:
      - "vectors/${vars.lang}.fasttext.vec"
