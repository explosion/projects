title: 'NEL Benchmark'
description: "Pipeline for benchmarking NEL approaches (incl. candidate generation and entity disambiguation)."
spacy_version: ">=3.0.0,<3.5.0"
vars:
  run: "cg-default"
  language: "en"
  config: "nel.cfg"
  vectors_model: "en_core_web_lg"
  version: "0.0.5"
  dataset: "mewsli_9"
  gpu_id: ""
  download_all_wiki_assets: ""  # "--extra" to download full Wiki dumps.
  filter: "True"  # Whether to only use parts of Wiki data and corpus containing filter terms.
  training_max_steps: 1000
  eval_highlight_metric: "F"  # one of ("F", "r", "p")

directories: ["assets", "training", "configs", "scripts", "corpora", "evaluation"]

workflows:
  all:
    - download_mewsli9
    - download_model
    - wikid_clone
    - preprocess
    - wikid_download_assets
    - wikid_parse
    - wikid_create_kb
    - parse_corpus
    - compile_corpora
    - train
    - evaluate
    - compare_evaluations
  training:
    - train
    - evaluate

commands:
  - name: download_mewsli9
    help: Download Mewsli-9 dataset.
    script:
      - bash scripts/datasets/download_mewsli-9.sh
    outputs:
      - assets/mewsli_9/raw

  - name: download_model
    help: "Download a model with pretrained vectors and NER component."
    script:
      - "python -m spacy download ${vars.vectors_model}"

  - name: wikid_clone
    help: Clone `wikid` to prepare Wiki database and `KnowledgeBase`.
    script:
      - git clone https://github.com/explosion/wikid.git --branch main
      - pip install -r wikid/requirements.txt
    outputs:
      - wikid

  - name: preprocess
    help: Preprocess and clean corpus data.
    script:
      - "env PYTHONPATH=. python ./scripts/clean_data.py ${vars.dataset} ${vars.language}"
    deps:
      - "assets/${vars.dataset}/raw"
    outputs:
      - "assets/${vars.dataset}/clean"

  - name: wikid_download_assets
    help: "Download Wikipedia dumps. This can take a long time if you're not using the filtered dumps!"
    script:
      - "spacy project assets wikid ${vars.download_all_wiki_assets}"
    outputs:
      - "wikid/assets/"

  - name: wikid_parse
    help: "Parse Wikipedia dumps. This can take a long time if you're not using the filtered dumps!"
    script:
      - "spacy project run parse wikid --vars.language ${vars.language} --vars.filter True"
    outputs:
      - "wikid/output/${vars.language}/wiki.sqlite3"

  - name: wikid_create_kb
    help: "Create the knowledge base and write it to file."
    script:
      - "spacy project run create_kb wikid --vars.language ${vars.language} --vars.vectors_model ${vars.vectors_model}"
    deps:
      - "wikid/output/${vars.language}/wiki.sqlite3"
    outputs_no_cache:
      - "wikid/output/${vars.language}/kb"
      - "wikid/output/${vars.language}/nlp"

  - name: parse_corpus
    help: "Parse corpus to generate entity and annotation lookups used for corpora compilation."
    script:
      - "env PYTHONPATH=. python ./scripts/parse_corpus.py ${vars.dataset} ${vars.language}"
    deps:
      - "assets/${vars.dataset}/clean"
      - "wikid/output/${vars.language}/wiki.sqlite3"
    outputs:
      - "assets/${vars.dataset}/entities.pkl"
      - "assets/${vars.dataset}/entities_failed_lookup.pkl"
      - "assets/${vars.dataset}/annotations.pkl"

  - name: compile_corpora
    help: "Compile corpora, separated in train/dev/test sets."
    script:
      - "env PYTHONPATH=. python ./scripts/compile_corpora.py ${vars.dataset} ${vars.language} ${vars.filter}"
    deps:
      - "assets/${vars.dataset}/entities.pkl"
      - "assets/${vars.dataset}/entities_failed_lookups.pkl"
      - "assets/${vars.dataset}/annotations.pkl"
      - "wikid/output/${vars.language}/kb"
      - "wikid/output/${vars.language}/nlp"
      - "configs/datasets.yml"
    outputs:
      - "corpora/${vars.dataset}/train.spacy"
      - "corpora/${vars.dataset}/dev.spacy"
      - "corpora/${vars.dataset}/test.spacy"

  - name: train
    help: "Train a new Entity Linking component. Pass --vars.gpu_id GPU_ID to train with GPU. Training with some datasets may take a long time!"
    script:
      - "bash scripts/train.sh ${vars.dataset} '${vars.run}' ${vars.language} ${vars.config} ${vars.training_max_steps} ${vars.gpu_id}"
    outputs:
      - "training/${vars.dataset}/${vars.run}"
    deps:
      - "wikid/output/${vars.language}/kb"
      - "wikid/output/${vars.language}/nlp"
      - "corpora/${vars.dataset}/train.spacy"
      - "corpora/${vars.dataset}/dev.spacy"

  - name: evaluate
    help: "Evaluate on the test set."
    script:
      - "env PYTHONPATH=. python ./scripts/evaluate.py ${vars.dataset} '${vars.run}' ${vars.language}"
    deps:
      - "training/${vars.dataset}/${vars.run}/model-best"
      - "wikid/output/${vars.language}/nlp"
      - "corpora/${vars.dataset}/dev.spacy"
    outputs:
      - "evaluation/${vars.dataset}"

  - name: compare_evaluations
    help: "Compare available set of evaluation runs."
    script:
      - "env PYTHONPATH=. python ./scripts/compare_evaluations.py ${vars.dataset} ${vars.language} --highlight-criterion ${vars.eval_highlight_metric}"
    deps:
      - "evaluation/${vars.dataset}"

  - name: delete_wiki_db
    help: "Deletes SQLite database generated in step wiki_parse with data parsed from Wikidata and Wikipedia dump."
    script:
      - "rm -f wikid/output/${vars.language}/wiki.sqlite3"
    deps:
      - "wikid/output/${vars.language}/wiki.sqlite3"

  - name: clean
    help: "Remove intermediate files for specified dataset and language (excluding Wiki resources and database)."
    script:
      - "rm -rf training/${vars.dataset}"
      - "rm -rf corpora/${vars.dataset}"
      - "rm -rf assets/${vars.dataset}"
      - "rm -rf evaluation/${vars.dataset}"
      - "rm -rf wikid/output/${vars.language}"

