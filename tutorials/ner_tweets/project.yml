title: "Detecting people entities in tweets (Named Entity Recognition)"
description: |
  This project demonstrates how to improve spaCy's pretrained models by
  augmenting the training data and adapting it to a different domain.

spacy_version: ">=3.3.0,<4.0.0"

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  config: "config.cfg"
  config_no_augmenter: "config_no_augmenter.cfg"
  name: "ner_tweets"
  version: "1.0.0"

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["assets", "corpus", "scripts", "metrics"]

# Assets that should be downloaded or available in the directory. We're shipping
# them with the project, so they won't have to be downloaded. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "assets/train_raw.jsonl"
    description: "The training dataset to augment. Later we'll divide this further to create a dev set"
  - dest: "assets/test_annotated.jsonl"
    description: "The held-out test dataset to evaluate our output against."
  - dest: "assets/btc.tar.gz"
    description: "A model trained on the Broad Twitter Corpus to help in annotation (63.7 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/btc.tar.gz"
  - dest: "assets/crunchbase.json.gz"
    description: "A list of crunchbase entities to aid the gazetteer annotator (8.56 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/crunchbase.json.gz"
  - dest: "assets/wikidata_tokenised.json.gz"
    description: "A list of wikipedia entities to aid the gazetteer annotator (21.1 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/wikidata_tokenised.json.gz"
  - dest: "assets/first_names.json"
    description: "A list of first names to help our heuristic annotator"
    url: "https://raw.githubusercontent.com/NorskRegnesentral/skweak/main/data/first_names.json"
  - dest: "assets/en_orth_variants.json"
    description: "Orth variants to use for data augmentation"
    url: "https://raw.githubusercontent.com/explosion/spacy-lookups-data/master/spacy_lookups_data/data/en_orth_variants.json"
    

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  all:
    - download
    - preprocess
    - decompress
    - augment
    - train
    - train-with-augmenter
    - evaluate
  setup:
    - download
    - preprocess
    - decompress
  finetune:
    - augment
    - train
    - train-with-augmenter
    - evaluate

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "download"
    help: "Download models"
    script:
      - "python -m spacy download en_core_web_lg"
  - name: "preprocess"
    help: "Convert raw inputs into spaCy's binary format"
    script:
      - "python scripts/preprocess.py assets/train_raw.jsonl corpus/train_raw.spacy --text-only"
      - "python scripts/preprocess.py assets/test_annotated.jsonl corpus/test_annotated.spacy"
    deps:
      - "assets/train_raw.jsonl"
      - "assets/test_annotated.jsonl"
    outputs:
      - "corpus/train_raw.spacy"
      - "corpus/test_annotated.spacy"

  - name: "decompress"
    help: "Decompress relevant assets that will be used latter by our weak supervision model"
    script:
      - "python scripts/decompress.py assets/crunchbase.json.gz assets/crunchbase.json"
      - "python scripts/decompress.py assets/wikidata_tokenised.json.gz assets/wikidata_tokenised.json"
      - "python scripts/decompress.py assets/btc.tar.gz assets/"
    deps:
      - "assets/crunchbase.json.gz"
      - "assets/wikidata_tokenised.json.gz"
      - "assets/btc.tar.gz"
    outputs:
      - "assets/crunchbase.json"
      - "assets/wikidata_tokenised.json"
      - "assets/data/btc/"

  - name: "augment"
    help: "Augment an input dataset via weak supervision then split it into training and evaluation datasets"
    script:
      - "python -m scripts.augment corpus/train_raw.spacy assets/hmm.pkl corpus/train_aug.spacy corpus/dev_aug.spacy"
    deps:
      - "corpus/train_raw.spacy"
      - "assets/crunchbase.json"
      - "assets/wikidata_tokenised.json"
      - "assets/first_names.json"
      - "assets/data/btc/"
    outputs:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
      - "assets/hmm.pkl"

  - name: "train"
    help: "Train a named entity recognition model"
    script:
      - "python -m spacy train configs/${vars.config_no_augmenter} --initialize.vectors en_core_web_lg --output training/spacy-no-augmenter --paths.train corpus/train_aug.spacy --paths.dev corpus/dev_aug.spacy"
    deps:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
    outputs:
      - "training/spacy-no-augmenter/model-best"

  - name: "train-with-augmenter"
    help: "Train a named entity recognition model with a spaCy built-in augmenter"
    script:
      - "python -m spacy train configs/${vars.config} --initialize.vectors en_core_web_lg --output training/spacy-augmenter/ --paths.train corpus/train_aug.spacy --paths.dev corpus/dev_aug.spacy"
    deps:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
    outputs:
      - "training/spacy-augmenter/model-best"

  - name: "evaluate"
    help: "Evaluate various experiments and export the computed metrics"
    script: 
      - "python -m spacy evaluate en_core_web_lg corpus/test_annotated.spacy --output metrics/baseline.json"
      - "python -m spacy evaluate training/spacy-no-augmenter/model-best corpus/test_annotated.spacy --output metrics/model_aug.json"
      - "python -m spacy evaluate training/spacy-augmenter/model-best corpus/test_annotated.spacy --output metrics/model_aug_spacy.json"
    deps:
      - "corpus/test_annotated.spacy"
      - "training/spacy-no-augmenter/model-best"
      - "training/spacy-augmenter/model-best"
    outputs:
      - "metrics/baseline.json"
      - "metrics/model_aug.json"
      - "metrics/model_aug_spacy.json"

  - name: "package"
    help: "Package the trained model so it can be installed"
    script: 
      - "python -m spacy package training/spacy-no-augmenter/model-best packages --name ${vars.name} --version ${vars.version} --force"
    deps:
      - "training/spacy-no-augmenter/model-best"
    outputs_no_cache:
      - "packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}.tar.gz"

