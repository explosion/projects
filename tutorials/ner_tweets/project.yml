title: "Detecting people entities in tweets (Named Entity Recognition)"
description: |
  This project demonstrates how to improve spaCy's pretrained models by
  augmenting the training data and adapting it to a different domain.

  **Weak supervision** is the practice of using imprecise annotators to label a
  dataset. These annotators take in the form of *labelling functions*, and may
  contain a single heuristic, a rule, or a model trained from related data.

  Once these functions are defined, we annotate the dataset and fit a Hidden Markov
  Model (HMM) to "pool" our annotators together. The result is a single, unified
  model that contains the pooled knowledge of each annotator. We can then use
  this unified model to re-annotate our entire training dataset.

  In this example, we will be using `skweak` as our weak supervision framework.
  It contains primitives that allows us to define our own labelling functions. 
  Throughout this project, we will be using the following annotators:
  - **A model-based annotator based on `en_core_web_lg`**: we can use existing
      spaCy models to bootstrap our annotations.
  - **A model-based annotator trained on the Broad Twitter Corpus** (BTC): 
      this gives us an edge due to having similar domains.
  - **A gazetteer-based annotator from Crunchbase**: a gazetteer can be thought of
      simply as a list of entities. In this case, we obtain all person names from
      Crunchbase to create an annotator of business personalities.
  - **A heuristic for finding proper names**: a simple implementation that checks
      for proper names in a given tweet. If there are two proper names joined by
      a prefix ("**van** Gogh", "**del** Pilar", "**de** Castro", etc.), then
      it's most likely a person's name. 
  - **A heuristic for finding full names based on a list**: we obtained a list of
      first names from skweak's data utilities, and use that to condition our
      search. If a full name is found, and a proper name comes after it, then
      it's most likely a person's name.


# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  config: "config.cfg"
  config_no_augmenter: "config_no_augmenter.cfg"
  name: "ner_tweets"
  version: "0.0.0"

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["assets", "corpus", "scripts", "metrics"]

# Assets that should be downloaded or available in the directory. We're shipping
# them with the project, so they won't have to be downloaded. But the
# 'project assets' command still lets you verify that the checksums match.
assets:
  - dest: "assets/train_raw.jsonl"
    description: "The training dataset to augment. Later we'll divide this further to create a dev set"
  - dest: "assets/test_annotated.jsonl"
    description: "The held-out test dataset to evaluate our output against."
  - dest: "assets/btc.tar.gz"
    description: "A model trained on the Broad Twitter Corpus to help in annotation (63.7 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/btc.tar.gz"
  - dest: "assets/crunchbase.json.gz"
    description: "A list of crunchbase entities to aid the gazetteer annotator (8.56 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/crunchbase.json.gz"
  - dest: "assets/wikidata_tokenised.json.gz"
    description: "A list of wikipedia entities to aid the gazetteer annotator (21.1 MB)"
    url: "https://github.com/NorskRegnesentral/skweak/releases/download/0.2.8/wikidata_tokenised.json.gz"
  - dest: "assets/first_names.json"
    description: "A list of first names to help our heuristic annotator"
    url: "https://raw.githubusercontent.com/NorskRegnesentral/skweak/main/data/first_names.json"
  - dest: "assets/en_orth_variants.json"
    description: "Orth variants to use for data augmentation"
    url: "https://raw.githubusercontent.com/explosion/spacy-lookups-data/master/spacy_lookups_data/data/en_orth_variants.json"
    

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  all:
    - install
    - preprocess
    - decompress
    - augment
    - train
    - evaluate
  setup:
    - install
    - preprocess
    - decompress
  finetune:
    - augment
    - train
    - evaluate

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "install"
    help: "Install dependencies"
    script: 
      - "pip3 install -r requirements.txt"
      - "python -m spacy download en_core_web_lg"
  - name: "preprocess"
    help: "Convert raw inputs into spaCy's binary format"
    script:
      - "python scripts/preprocess.py assets/train_raw.jsonl corpus/train_raw.spacy --text-only"
      - "python scripts/preprocess.py assets/test_annotated.jsonl corpus/test_annotated.spacy"
    deps:
      - "assets/train_raw.jsonl"
      - "assets/test_annotated.jsonl"
    outputs:
      - "corpus/train_raw.spacy"
      - "corpus/test_annotated.spacy"

  - name: "decompress"
    help: "Decompress relevant assets that will be used latter by our weak supervision model"
    script:
      - "python scripts/decompress.py assets/crunchbase.json.gz assets/crunchbase.json"
      - "python scripts/decompress.py assets/wikidata_tokenised.json.gz assets/wikidata_tokenised.json"
      - "python scripts/decompress.py assets/btc.tar.gz assets/"
    deps:
      - "assets/crunchbase.json.gz"
      - "assets/wikidata_tokenised.json.gz"
      - "assets/btc.tar.gz"
    outputs:
      - "assets/crunchbase.json"
      - "assets/wikidata_tokenised.json"
      - "assets/data/btc/"

  - name: "augment"
    help: "Augment an input dataset via weak supervision then split it into training and evaluation datasets"
    script:
      - "python -m scripts.augment corpus/train_raw.spacy assets/hmm.pkl corpus/train_aug.spacy corpus/dev_aug.spacy"
    deps:
      - "corpus/train_raw.spacy"
      - "assets/crunchbase.json"
      - "assets/wikidata_tokenised.json"
      - "assets/first_names.json"
      - "assets/data/btc/"
    outputs:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
      - "assets/hmm.pkl"

  - name: "train"
    help: "Train a named entity recognition model"
    script:
      - "python -m spacy train configs/${vars.config_no_augmenter} --initialize.vectors en_core_web_lg --output training/spacy-no-augmenter --paths.train corpus/train_aug.spacy --paths.dev corpus/dev_aug.spacy"
    deps:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
    outputs:
      - "training/spacy-no-augmenter/model-best"

  - name: "train-with-augmenter"
    help: "Train a named entity recognition model with a spaCy built-in augmenter"
    script:
      - "python -m spacy train configs/${vars.config} --initialize.vectors en_core_web_lg --output training/spacy-augmenter/ --paths.train corpus/train_aug.spacy --paths.dev corpus/dev_aug.spacy"
    deps:
      - "corpus/train_aug.spacy"
      - "corpus/dev_aug.spacy"
    outputs:
      - "training/spacy-augmenter/model-best"

  - name: "evaluate"
    help: "Evaluate various experiments and export the computed metrics"
    script: 
      - "python -m spacy evaluate en_core_web_lg corpus/test_annotated.spacy --output metrics/baseline.json"
      - "python -m spacy evaluate training/spacy-no-augmenter/model-best corpus/test_annotated.spacy --output metrics/model_aug.json"
      - "python -m spacy evaluate training/spacy-augmenter/model-best corpus/test_annotated.spacy --output metrics/model_aug_spacy.json"
    deps:
      - "corpus/test_annotated.spacy"
      - "training/spacy-no-augmenter/model-best"
      - "training/spacy-augmenter/model-best"
    outputs:
      - "metrics/baseline.json"
      - "metrics/model_aug.json"
      - "metrics/model_aug_spacy.json"

  - name: "package"
    help: "Package the trained model so it can be installed"
    script: 
      - "python -m spacy package training/spacy-no-augmenter/model-best packages --name ${vars.name} --version ${vars.version} --force"
    deps:
      - "training/spacy-no-augmenter/model-best"
    outputs_no_cache:
      - "packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}.tar.gz"

