title: "Training a named-entity recognition (NER) with multiple trials"
description: |
  This project demonstrates how to train a spaCy pipeline with multiple trials.
  It trains a named-entity recognition (NER) model on the ConLL 2003 English
  dataset.  Having multiple trials is useful for experiments, especially if we
  want to account for variance and *dependency* on a random seed. 

  Under the hood, the training script in `scripts/train_with_trials.py`
  generates a random seed per trial, and runs the `train` command as usual.  You
  can find the trained model per trial in `training/{seed}/`.

  At evaluation, you can pass a directory containing all the models for each
  trial.  This process is demonstrated in `scripts/evaluate_with_trials.py`.
  This will result to multiple `metrics/scores.json` files that you can
  summarize using the `scripts/summarize_results.py` script (only written for
  this particular dataset).

directories:
  - assets
  - configs
  - corpus
  - metrics
  - training

vars:
  gpu_id: 0
  trials: 5
  config: "ner_accuracy.cfg"
  max_steps: 20000

workflows:
  all:
    - install
    - preprocess
    - convert
    - train
    - evaluate

assets:
  - dest: "assets/raw-en-conll-train.iob"
    description: "CoNLL 2003 (en) training dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/train.conllu
  - dest: "assets/raw-en-conll-dev.iob"
    description: "CoNLL 2003 (en) dev dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/val.conllu
  - dest: "assets/raw-en-conll-test.iob"
    description: "CoNLL 2003 (en) test dataset"
    url: https://github.com/Babelscape/wikineural/blob/master/data/conll/en/test.conllu

commands:
  - name: "install"
    help: "Install spaCy models"
    script:
      - python -m spacy download en_core_web_lg

  - name: "preprocess"
    help: "Preprocess the ConLL 2003 dataset to remove indices and update delimiters."
    script:
      - python -m scripts.preprocess assets/raw-en-conll-train.iob assets/en-conll-train.iob
      - python -m scripts.preprocess assets/raw-en-conll-dev.iob assets/en-conll-dev.iob
      - python -m scripts.preprocess assets/raw-en-conll-test.iob assets/en-conll-test.iob
    deps:
      - assets/raw-en-conll-train.iob
      - assets/raw-en-conll-dev.iob
      - assets/raw-en-conll-test.iob
    outputs:
      - assets/en-conll-train.iob
      - assets/en-conll-dev.iob
      - assets/en-conll-test.iob

  - name: "convert"
    help: "Convert IOB dataset into the spaCy format."
    script:
      - python -m spacy convert assets/en-conll-train.iob corpus/
      - python -m spacy convert assets/en-conll-dev.iob corpus/
      - python -m spacy convert assets/en-conll-test.iob corpus/
    deps:
      - assets/en-conll-train.iob
      - assets/en-conll-dev.iob
      - assets/en-conll-test.iob
    outputs:
      - corpus/en-conll-train.spacy
      - corpus/en-conll-dev.spacy
      - corpus/en-conll-test.spacy

  - name: "train"
    help: "Train a named-entity recognition (NER) model for a multiple number of trials."
    script:
      - >-
        python -m scripts.train_with_trials
        configs/${vars.config}
        --n-trials ${vars.trials}
        --nlp.lang en
        --output training/
        --paths.train corpus/en-conll-train.spacy
        --paths.dev corpus/en-conll-dev.spacy
        --training.max_steps ${vars.max_steps}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/en-conll-train.spacy
      - corpus/en-conll-dev.spacy
    outputs:
      - training

  - name: "evaluate"
    help: "Evaluate all models for each trial, then summarize the results."
    script:
      - >-
        python -m scripts.evaluate_with_trials
        training/
        corpus/en-conll-test.spacy
        --output-dir metrics/
        --gpu-id ${vars.gpu_id}

  - name: "clean"
    help: "Remove cached files"
    script:
      - rm -rf training/
